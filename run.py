# -*- coding: UTF-8 -*-
try:
    import os
    import cv2
    import time
    import torch
    import numpy as np
    import torchvision.transforms as transforms
    from datetime import datetime
    from PIL import ImageFont, ImageDraw, Image
    from module.colors import *
    from module.trainer import *
    from deepface import DeepFace
    from torchvision.models.detection import fasterrcnn_resnet50_fpn
except ImportError:
    raise ImportError("ğŸ¥¹æ— æ³•å®‰è£…é…ä»¶")
finally:
    pass


class çœ¼è¿¹AI:

    def __init__(self) -> None:
        """
        åˆå§‹åŒ–çœ¼è¿¹AIç±»ã€‚

        Attributes:
            åç§° (str): çœ¼è¿¹AIçš„åç§°ã€‚
            ç²—ç»† (float): ç”»ç¬”ç²—ç»†ã€‚
            å­—ä½“è·¯å¾„ (str): å­—ä½“æ–‡ä»¶è·¯å¾„ã€‚
            çœ¼éƒ¨åˆ†ç±»å™¨: OpenCVçœ¼éƒ¨åˆ†ç±»å™¨å¯¹è±¡ã€‚
            çœ¼ç›åˆ†ç±»å™¨: OpenCVçœ¼ç›åˆ†ç±»å™¨å¯¹è±¡ã€‚
            æ•è·: OpenCVè§†é¢‘æ•è·å¯¹è±¡ã€‚
            çœ¼ç›ä½ç½® (dict): æŒ‡ç¤ºçœ¼ç›ä½ç½®çš„æ–‡æœ¬å’Œé¢œè‰²çš„å­—å…¸ã€‚
            åˆå§‹å¸§æ•° (float): åˆå§‹å¸§æ•°å€¼ç”¨äºè®¡ç®—çœ¨çœ¼ã€‚
        """
        super(çœ¼è¿¹AI, self).__init__()

        self.åç§°: str = "çœ¼è¿¹AI"
        self.ç²—ç»†: float = 0.5
        self.å­—ä½“è·¯å¾„: str = "assets/YRDZST Medium.ttf"
        self.enforce_detection: bool = False

        # åŠ è½½é¢„è®­ç»ƒçš„çœ¼éƒ¨å’Œçœ¼ç›åˆ†ç±»å™¨
        self.çœ¼éƒ¨åˆ†ç±»å™¨ = cv2.CascadeClassifier(
            cv2.data.haarcascades + "haarcascade_frontalface_default.xml"
        )

        self.çœ¼ç›åˆ†ç±»å™¨ = cv2.CascadeClassifier(
            cv2.data.haarcascades + "haarcascade_eye.xml"
        )

        # åˆå§‹åŒ–è§†é¢‘æ•è·
        self.æ•è· = cv2.VideoCapture(0)
        self.å¯ç”¨æ‘„åƒå¤´ = []
        self.ç´¢å¼• = 0

        # å®šä¹‰çœ¼ç›ä½ç½®å¯¹åº”çš„æ–‡æœ¬å’ŒçŸ©å½¢é¢œè‰²çš„å­—å…¸
        self.çœ¼ç›ä½ç½®: dict = {
            "å·¦": ("å‘å·¦çœ‹", (255, 0, 0)),  # çº¢è‰²
            "å³": ("å‘å³çœ‹", (255, 0, 0)),  # è“è‰²
            "æ­£": ("æ­£è§†", (0, 255, 0)),  # ç»¿è‰²
        }

        self.åˆå§‹å¸§æ•°: object = None
        self.æ˜¾ç¤ºæ–‡å­—: bool = False
        self.train: bool = False

        self.äººè„¸é…ç½®: str = "assets/training_bin/opencv_face_detector.pbtxt"
        self.äººè„¸æ¨¡å‹: str = "assets/training_bin/opencv_face_detector_uint8.pb"
        self.å¹´é¾„é…ç½®: str = "assets/training_bin/age_deploy.prototxt"
        self.å¹´é¾„æ¨¡å‹: str = "assets/training_bin/age_net.caffemodel"
        self.æ€§åˆ«é…ç½®: str = "assets/training_bin/gender_deploy.prototxt"
        self.æ€§åˆ«æ¨¡å‹: str = "assets/training_bin/gender_net.caffemodel"

        self.ç‰©ä½“æ£€æµ‹æ¨¡å‹åç§°: str = "object_detection_model"

        """
        self.object_detection_model = fasterrcnn_resnet50_fpn(pretrained=False)
        self.object_detection_model.load_state_dict(torch.load("{}.pth".format(self.ç‰©ä½“æ£€æµ‹æ¨¡å‹åç§°)))
        self.object_detection_model.eval()
        """

    def è®°å½•ä¿¡æ¯(self, ä¿¡æ¯: str) -> None:
        """
        è®°å½•ä¿¡æ¯åˆ°æ§åˆ¶å°ã€‚

        Args:
            ä¿¡æ¯ (str): è¦è®°å½•çš„ä¿¡æ¯ã€‚
        """
        ç°åœ¨æ—¶é—´: str = str(datetime.now().strftime("%H:%M:%S"))
        print(light_green + f"{self.åç§°} |> [{ç°åœ¨æ—¶é—´}] {ä¿¡æ¯}")

    def è®¡ç®—çœ¨çœ¼(self, çœ¼ç›) -> float:
        """
        è®¡ç®—çœ¨çœ¼è·ç¦»ã€‚

        Args:
            çœ¼ç› (list): åŒ…å«çœ¼ç›ä¿¡æ¯çš„åˆ—è¡¨ã€‚

        Returns:
            float: çœ¨çœ¼è·ç¦»ã€‚
        """
        if len(çœ¼ç›) == 2:
            (ex1, ey1, ew1, eh1), (ex2, ey2, ew2, eh2) = çœ¼ç›

            # è®¡ç®—ä¸¤åªçœ¼ç›çš„ä¸­å¿ƒç‚¹
            ä¸­å¿ƒ_x1 = ex1 + ew1 // 2
            ä¸­å¿ƒ_y1 = ey1 + eh1 // 2
            ä¸­å¿ƒ_x2 = ex2 + ew2 // 2
            ä¸­å¿ƒ_y2 = ey2 + eh2 // 2

            # è®¡ç®—ä¸¤åªçœ¼ç›ä¹‹é—´çš„è·ç¦»
            return np.sqrt((ä¸­å¿ƒ_x2 - ä¸­å¿ƒ_x1) ** 2 + (ä¸­å¿ƒ_y2 - ä¸­å¿ƒ_y1) ** 2)

    def è¿è¡Œ(self) -> None:
        global å·¦çœ¼
        global å³çœ¼
        global é¢œè‰²
        global å­—ä½“
        global æ–¹å‘æ–‡æœ¬
        global ç”¨æˆ·æƒ…ç»ª
        global æ€§åˆ«æ ‡ç­¾

        è®¾å¤‡åç§° = self.æ•è·.get(cv2.CAP_PROP_POS_MSEC)

        if not os.path.exists(f"{self.ç‰©ä½“æ£€æµ‹æ¨¡å‹åç§°}.pth"):
            _è®­ç»ƒå™¨_ = è®­ç»ƒå™¨(æ–‡ä»¶åç§°=self.ç‰©ä½“æ£€æµ‹æ¨¡å‹åç§°)
            _è®­ç»ƒå™¨_.è¿è¡Œ(è®­ç»ƒæ•°æ®="assets/train", æµ‹è¯•æ•°æ®="assets/test")
        else:
            pass

        self.ç‰©ä½“æ£€æµ‹æ¨¡å‹ = torch.load(f"{self.ç‰©ä½“æ£€æµ‹æ¨¡å‹åç§°}.pth")

        self.å¯ç”¨æ‘„åƒå¤´.append((self.ç´¢å¼•, è®¾å¤‡åç§°))
        self.ç´¢å¼• += 1

        for self.ç´¢å¼•, è®¾å¤‡åç§° in self.å¯ç”¨æ‘„åƒå¤´:
            self.è®°å½•ä¿¡æ¯(
                ä¿¡æ¯=f"{self.æ•è·.__str__()} | æ‘„åƒå¤´ {self.ç´¢å¼•}: {è®¾å¤‡åç§°}"
            )

        if not os.path.exists(self.å­—ä½“è·¯å¾„):
            self.è®°å½•ä¿¡æ¯(ä¿¡æ¯=f"[x] {self.å­—ä½“è·¯å¾„} ä¸å­˜åœ¨! [x]")

        å­—ä½“ = ImageFont.truetype(self.å­—ä½“è·¯å¾„, 30)

        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        äººè„¸ç½‘ç»œ = cv2.dnn.readNetFromTensorflow(self.äººè„¸æ¨¡å‹, self.äººè„¸é…ç½®)
        æ€§åˆ«ç½‘ç»œ = cv2.dnn.readNetFromCaffe(self.æ€§åˆ«é…ç½®, self.æ€§åˆ«æ¨¡å‹)
        å¹´é¾„ç½‘ç»œ = cv2.dnn.readNetFromCaffe(self.å¹´é¾„é…ç½®, self.å¹´é¾„æ¨¡å‹)

        while True:
            # ä»æ‘„åƒå¤´è¯»å–å¸§
            ret, f = self.æ•è·.read()

            å¸§ = cv2.flip(f, 1)

            # å°†å¸§è½¬æ¢ä¸ºç°åº¦å›¾ä»¥è¿›è¡Œçœ¼éƒ¨æ£€æµ‹
            ç°åº¦ = cv2.cvtColor(å¸§, cv2.COLOR_BGR2GRAY)

            face_region_of_interest = cv2.cvtColor(ç°åº¦, cv2.COLOR_GRAY2RGB)

            # åœ¨å¸§ä¸­æ£€æµ‹çœ¼ç›
            çœ¼ç› = self.çœ¼ç›åˆ†ç±»å™¨.detectMultiScale(ç°åº¦)

            # åœ¨å¸§ä¸­æ£€æµ‹äººè„¸
            è„¸éƒ¨ = self.çœ¼éƒ¨åˆ†ç±»å™¨.detectMultiScale(ç°åº¦)

            é¢œè‰² = (255, 50, 0)

            for fx, fy, fw, fh in è„¸éƒ¨:
                # ç»˜åˆ¶åœ†å½¢åŒºåŸŸä»¥è¡¨ç¤ºè„¸éƒ¨ | ç»˜åˆ¶å…·æœ‰åœ†è§’çš„çŸ©å½¢
                åŠå¾„ = int(min(fw, fh) / 2)

                cv2.rectangle(
                    å¸§,
                    (int(fx), int(fy)),
                    (int(fx + fw), int(fy + fh)),
                    é¢œè‰²,
                    thickness=2,
                )

                # ä¸ºäººè„¸æ£€æµ‹å‡†å¤‡è¾“å…¥å›¾åƒ
                å›¾åƒblob = cv2.dnn.blobFromImage(
                    å¸§, 1.0, (300, 300), [104, 117, 123], False, False
                )

                # æ‰§è¡Œäººè„¸æ£€æµ‹
                äººè„¸ç½‘ç»œ.setInput(å›¾åƒblob)
                æ£€æµ‹ç»“æœ = äººè„¸ç½‘ç»œ.forward()

                # éå†æ£€æµ‹åˆ°çš„äººè„¸
                for i in range(æ£€æµ‹ç»“æœ.shape[2]):
                    ç½®ä¿¡åº¦ = æ£€æµ‹ç»“æœ[0, 0, i, 2]

                    # æ ¹æ®ç½®ä¿¡åº¦é˜ˆå€¼è¿‡æ»¤æ‰å¼±æ£€æµ‹ç»“æœ
                    if ç½®ä¿¡åº¦ > 0.5:
                        # æå–è¾¹ç•Œæ¡†åæ ‡
                        è¾¹ç•Œæ¡† = æ£€æµ‹ç»“æœ[0, 0, i, 3:7] * np.array(
                            [å¸§.shape[1], å¸§.shape[0], å¸§.shape[1], å¸§.shape[0]]
                        )

                        (èµ·å§‹X, èµ·å§‹Y, ç»“æŸX, ç»“æŸY) = è¾¹ç•Œæ¡†.astype("int")

                        # æå–äººè„¸åŒºåŸŸ
                        äººè„¸åŒºåŸŸ = å¸§[èµ·å§‹Y:ç»“æŸY, èµ·å§‹X:ç»“æŸX]

                        # ä¸ºå¹´é¾„åˆ†ç±»å‡†å¤‡äººè„¸å›¾åƒ
                        äººè„¸blob = cv2.dnn.blobFromImage(
                            äººè„¸åŒºåŸŸ,
                            1.0,
                            (227, 227),
                            (78.4263377603, 87.7689143744, 114.895847746),
                            swapRB=False,
                        )

                        # æ‰§è¡Œå¹´é¾„åˆ†ç±»
                        å¹´é¾„ç½‘ç»œ.setInput(äººè„¸blob)
                        å¹´é¾„é¢„æµ‹ = å¹´é¾„ç½‘ç»œ.forward()

                        # ä¸ºæ€§åˆ«åˆ†ç±»å‡†å¤‡äººè„¸å›¾åƒ
                        äººè„¸blob = cv2.dnn.blobFromImage(
                            äººè„¸åŒºåŸŸ,
                            1.0,
                            (227, 227),
                            (78.4263377603, 87.7689143744, 114.895847746),
                            swapRB=False,
                        )

                        # æ‰§è¡Œæ€§åˆ«åˆ†ç±»
                        æ€§åˆ«ç½‘ç»œ.setInput(äººè„¸blob)
                        æ€§åˆ«é¢„æµ‹ = æ€§åˆ«ç½‘ç»œ.forward()

                        æ€§åˆ« = "Male" if æ€§åˆ«é¢„æµ‹[0][0] > æ€§åˆ«é¢„æµ‹[0][1] else "Female"

                        # åœ¨å¸§ä¸Šå åŠ æ€§åˆ«æ ‡ç­¾
                        æ€§åˆ«æ ‡ç­¾ = "{}: {:.2f}%".format(
                            æ€§åˆ«, max(æ€§åˆ«é¢„æµ‹[0][0], æ€§åˆ«é¢„æµ‹[0][1]) * 100
                        )

                        cv2.putText(
                            å¸§,
                            f"Gender: {æ€§åˆ«æ ‡ç­¾}",
                            (èµ·å§‹X, èµ·å§‹Y - 50),
                            cv2.FONT_HERSHEY_SIMPLEX,
                            1,
                            é¢œè‰²,
                            2,
                            cv2.LINE_AA,
                            False,
                        )

                        cv2.putText(
                            å¸§,
                            f"Identity : Unknown",
                            (èµ·å§‹X, èµ·å§‹Y - 90),
                            cv2.FONT_HERSHEY_COMPLEX_SMALL,
                            1,
                            é¢œè‰²,
                            2,
                            cv2.LINE_AA,
                            False,
                        )
                    else:
                        continue

            """

            # Convert frame to the format expected by the object detection model
            input_image = transforms.ToTensor()(å¸§).unsqueeze(0)

            # Perform object detection
            with torch.no_grad():
                predictions = self.object_detection_model(input_image)

            # Process the predictions and draw bounding boxes on the frame
            for box in predictions[0]['boxes']:
                box = [int(coord) for coord in box]
                cv2.rectangle(å¸§, (box[0], box[1]), (box[2], box[3]), (255, 0, 0), 2)

            cv2.imshow(self.åç§°, å¸§)

            """

            # éå†æ¯ä¸ªæ£€æµ‹åˆ°çš„çœ¼ç›
            for ex, ey, ew, eh in çœ¼ç›:
                # è®¡ç®—çœ¼ç›é¢ç§¯
                çœ¼ç›é¢ç§¯ = ew * eh

                # æ£€æŸ¥æ£€æµ‹åˆ°çš„å¯¹è±¡æ˜¯å¦æ˜¯äººçœ¼ï¼ŒåŸºäºé¢ç§¯
                if çœ¼ç›é¢ç§¯ > 1000 and çœ¼ç›é¢ç§¯ < 5000:

                    # è·å–çœ¼ç›ä¸­å¿ƒç‚¹åæ ‡
                    çœ¼ç›ä¸­å¿ƒ_x = ex + ew // 2

                    # è®¡ç®—çœ¼ç›ä¸­å¿ƒ
                    çœ¼ç›ä¸­å¿ƒ = (ex + ew // 2, ey + eh // 2)

                    # è®¡ç®—åœ†å½¢åŠå¾„
                    åŠå¾„ = max(ew, eh) // 2

                    # æ ¹æ®çœ¼ç›ä¸­å¿ƒç‚¹ä½ç½®åˆ¤æ–­æœå‘
                    if self.æ˜¾ç¤ºæ–‡å­—:

                        if çœ¼ç›ä¸­å¿ƒ_x < å¸§.shape[1] // 3:
                            æ–¹å‘æ–‡æœ¬ = "å‘å·¦çœ‹"  # å¦‚æœçœ¼ç›ä¸­å¿ƒåœ¨å¸§çš„å·¦ä¾§
                            é¢œè‰² = (255, 0, 0)  # çº¢è‰²

                        elif çœ¼ç›ä¸­å¿ƒ_x > 2 * å¸§.shape[1] // 3:
                            æ–¹å‘æ–‡æœ¬ = "å‘å³çœ‹"  # å¦‚æœçœ¼ç›ä¸­å¿ƒåœ¨å¸§çš„å³ä¾§
                            é¢œè‰² = (255, 0, 0)  # è“è‰²

                        else:
                            æ–¹å‘æ–‡æœ¬ = "æ­£è§†"  # å¦‚æœçœ¼ç›ä¸­å¿ƒåœ¨å¸§çš„ä¸­é—´éƒ¨åˆ†
                            é¢œè‰² = (0, 255, 0)  # ç»¿è‰²

                    else:
                        æ–¹å‘æ–‡æœ¬ = ""  # default

                    # ç”¨æˆ·æƒ…ç»ªã€Šåœ¨äººè„¸æ„Ÿå…´è¶£åŒºåŸŸæ‰§è¡Œæƒ…ç»ªåˆ†æã€‹
                    ç»“æœ = DeepFace.analyze(
                        face_region_of_interest,
                        actions=["emotion"],
                        enforce_detection=self.enforce_detection,
                    )

                    # ç¡®å®šä¸»è¦æƒ…ç»ª
                    æƒ…ç»ª: str = ç»“æœ[0]["dominant_emotion"]
                    æƒ…ç»ªå‡†ç¡®æ€§: float = ç»“æœ[0]["face_confidence"]

                    xRegion: float = ç»“æœ[0]["region"]["x"]
                    yRegion: float = ç»“æœ[0]["region"]["y"]
                    wRegion: float = ç»“æœ[0]["region"]["w"]
                    hRegion: float = ç»“æœ[0]["region"]["h"]

                    lEyes: list = ç»“æœ[0]["region"]["left_eye"]
                    rEyes: list = ç»“æœ[0]["region"]["right_eye"]

                    if lEyes is None and rEyes is None or xRegion == 0 and yRegion == 0:
                        print(pure_red + "[è­¦å‘Šï¼šè¯·ä¿æŒæ³¨æ„åŠ›é›†ä¸­åœ¨å‰æ–¹!!!] ")
                        self.warning(
                            å¸§,
                            "WARNING: PLEASE KEEP YOU EYE ON THE FRONT !!!",
                            1,
                            (0, 0, 255),
                            2,
                        )

                    # åœ¨çœ¼ç›å‘¨å›´ç»˜åˆ¶ä¸€ä¸ªçŸ©å½¢
                    # cv2.rectangle(å¸§, (ex, ey), (ex + ew, ey + eh), é¢œè‰², thickness=3)

                    # ç»˜åˆ¶åœ†å½¢
                    cv2.circle(å¸§, çœ¼ç›ä¸­å¿ƒ, åŠå¾„, é¢œè‰², thickness=2)

                    # åœ¨å›¾åƒä¸Šç»˜åˆ¶æ–¹å‘æ–‡æœ¬
                    å¸§_pil = Image.fromarray(cv2.cvtColor(å¸§, cv2.COLOR_BGR2RGB))
                    draw = ImageDraw.Draw(å¸§_pil)
                    draw.text((ex, ey - 30), æ–¹å‘æ–‡æœ¬, font=å­—ä½“, fill=é¢œè‰²)
                    å¸§ = cv2.cvtColor(np.array(å¸§_pil), cv2.COLOR_RGB2BGR)

                    self.è®°å½•ä¿¡æ¯(
                        ä¿¡æ¯=f"æ€§åˆ«: ({æ€§åˆ«æ ‡ç­¾}) | {æƒ…ç»ª}\t{æƒ…ç»ªå‡†ç¡®æ€§ * 100} % | [ å·¦çœ¼: {lEyes}, å³çœ¼: {rEyes} ] x: {xRegion}, y: {yRegion}, w: {wRegion}, h: {hRegion}".upper()
                    )

                    cv2.putText(
                        å¸§,
                        str(æƒ…ç»ª + f" {æƒ…ç»ªå‡†ç¡®æ€§ * 100} %").upper(),
                        (fx, fy - 10),
                        cv2.FONT_HERSHEY_COMPLEX_SMALL,
                        0.9,
                        é¢œè‰²,
                        1,
                        cv2.LINE_AA,
                    )

                    cv2.putText(
                        å¸§,
                        "YanJiAI",
                        (10, 30),
                        cv2.FONT_HERSHEY_SIMPLEX,
                        1,
                        é¢œè‰²,
                        2,
                        cv2.LINE_AA,
                    )

                    cv2.putText(
                        å¸§,
                        str(f"L: {lEyes}"),
                        (10, 55),
                        cv2.FONT_HERSHEY_COMPLEX_SMALL,
                        1,
                        é¢œè‰²,
                        2,
                        cv2.LINE_AA,
                    )

                    cv2.putText(
                        å¸§,
                        str(f"R: {rEyes}"),
                        (10, 75),
                        cv2.FONT_HERSHEY_COMPLEX_SMALL,
                        1,
                        é¢œè‰²,
                        2,
                        cv2.LINE_AA,
                    )

                    cv2.putText(
                        å¸§,
                        str(f"X: {xRegion}"),
                        (10, 95),
                        cv2.FONT_HERSHEY_COMPLEX_SMALL,
                        1,
                        é¢œè‰²,
                        2,
                        cv2.LINE_AA,
                    )

                    cv2.putText(
                        å¸§,
                        str(f"Y: {yRegion}"),
                        (10, 115),
                        cv2.FONT_HERSHEY_COMPLEX_SMALL,
                        1,
                        é¢œè‰²,
                        2,
                        cv2.LINE_AA,
                    )

                    cv2.putText(
                        å¸§,
                        str(f"W: {wRegion}"),
                        (10, 135),
                        cv2.FONT_HERSHEY_COMPLEX_SMALL,
                        1,
                        é¢œè‰²,
                        2,
                        cv2.LINE_AA,
                    )

                    cv2.putText(
                        å¸§,
                        str(f"H: {hRegion}"),
                        (10, 155),
                        cv2.FONT_HERSHEY_COMPLEX_SMALL,
                        1,
                        é¢œè‰²,
                        2,
                        cv2.LINE_AA,
                    )

                # è®¡ç®—çœ¨çœ¼
                if len(çœ¼ç›) == 2:
                    çœ¨çœ¼è·ç¦» = self.è®¡ç®—çœ¨çœ¼(çœ¼ç›)

                    if self.åˆå§‹å¸§æ•° is None:
                        self.åˆå§‹å¸§æ•° = çœ¨çœ¼è·ç¦»

                    else:
                        if çœ¨çœ¼è·ç¦» < self.åˆå§‹å¸§æ•° * 0.8:

                            å¸§_pil = Image.fromarray(
                                cv2.cvtColor(å¸§, cv2.COLOR_BGR2RGB)
                            )

                            draw = ImageDraw.Draw(å¸§_pil)
                            å¸§ = cv2.cvtColor(np.array(å¸§_pil), cv2.COLOR_RGB2BGR)

                            self.è®°å½•ä¿¡æ¯(ä¿¡æ¯="çœ¨çœ¼")

                    if self.æ˜¾ç¤ºæ–‡å­—:
                        self.è®°å½•ä¿¡æ¯(ä¿¡æ¯=æ–¹å‘æ–‡æœ¬)

            # æ˜¾ç¤ºå¸§
            cv2.imshow(self.åç§°, å¸§)

            # å¦‚æœæŒ‰ä¸‹ 'q' é”®ï¼Œåˆ™é€€å‡ºå¾ªç¯
            if cv2.waitKey(1) & 0xFF in [ord("q"), ord("z")]:
                exit(0)
                break

        # é‡Šæ”¾è§†é¢‘æ•è·
        self.æ•è·.release()
        cv2.destroyAllWindows()

    def warning(self, frame, text, font_scale, color, thickness):
        # å®šä¹‰å­—ä½“
        font = cv2.FONT_HERSHEY_COMPLEX_SMALL

        # è·å–æ–‡æœ¬å¤§å°
        text_size = cv2.getTextSize(text, font, font_scale, thickness)[0]

        # è®¡ç®—æ–‡æœ¬åœ¨å¸§åº•éƒ¨çš„ä½ç½®
        text_x = int((frame.shape[1] - text_size[0]) / 2)
        text_y = frame.shape[0] - 20  # æ ¹æ®éœ€è¦è°ƒæ•´æ­¤å€¼ä»¥è®¾ç½®è·ç¦»å¸§åº•éƒ¨çš„è·ç¦»

        # å°†æ–‡æœ¬æ”¾ç½®åœ¨å¸§åº•éƒ¨
        cv2.putText(
            frame,
            text,
            (text_x, text_y),
            font,
            font_scale,
            color,
            thickness,
            cv2.LINE_AA,
        )


if __name__ == "__main__":
    yanjiai: object = çœ¼è¿¹AI()
    yanjiai.è¿è¡Œ()
