# -*- coding: UTF-8 -*-
try:
    import os
    import tensorflow as tf
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
    from tensorflow.keras.optimizers import legacy
    from PIL import Image
except ImportError:
    raise ImportError("ğŸ¥¹æ— æ³•å®‰è£…é…ä»¶")
finally:
    pass

class Trainer:
    def __init__(self, æ–‡ä»¶åç§°: str) -> None:
        super(Trainer, self).__init__()

        self.æ–‡ä»¶åç§°: str = æ–‡ä»¶åç§°

        assert tf.__version__.startswith("2"), "This script requires TensorFlow 2.x."

        self.fill_mode: str = "nearest"

    def check_images(self, directory: str) -> None:
        for filename in os.listdir(directory):
            if filename.endswith((".png", ".jpg", ".jpeg")):
                try:
                    img = Image.open(os.path.join(directory, filename))
                    img.verify()  # Verify that it is, in fact, an image
                except (IOError, SyntaxError) as e:
                    print(f"Bad file: {filename}")  # Print out the names of corrupt files

    def è¿è¡Œ(self, æ•°æ®: str, æµ‹è¯•æ•°æ®: str) -> None:

        if not os.path.exists(æ•°æ®):
            raise FileNotFoundError(f"è®­ç»ƒæ•°æ®ç›®å½• '{æ•°æ®}' ä¸å­˜åœ¨.")
        
        if not os.path.exists(æµ‹è¯•æ•°æ®):
            raise FileNotFoundError(f"æµ‹è¯•æ•°æ®ç›®å½• '{æµ‹è¯•æ•°æ®}' ä¸å­˜åœ¨.")

        # Check images in directories
        self.check_images(æ•°æ®)
        self.check_images(æµ‹è¯•æ•°æ®)

        # åˆ›å»º ImageDataGenerator å®ä¾‹
        train_datagen = ImageDataGenerator(
            rescale=1./255,
            rotation_range=40,
            width_shift_range=0.2,
            height_shift_range=0.2,
            shear_range=0.2,
            zoom_range=0.2,
            horizontal_flip=True,
            fill_mode=self.fill_mode
        )

        val_datagen = ImageDataGenerator(rescale=1./255)

        # åŠ è½½è®­ç»ƒæ•°æ®
        train_generator = train_datagen.flow_from_directory(
            æ•°æ®,
            target_size=(150, 150),
            batch_size=32,
            class_mode="binary"  # ä½¿ç”¨ "categorical" è¿›è¡Œå¤šç±»åˆ†ç±»
        )

        # åŠ è½½éªŒè¯æ•°æ®
        val_generator = val_datagen.flow_from_directory(
            æµ‹è¯•æ•°æ®,
            target_size=(150, 150),
            batch_size=32,
            class_mode="binary"
        )

        # å®šä¹‰æ¨¡å‹
        model = Sequential([
            Conv2D(32, (3, 3), activation="relu", input_shape=(150, 150, 3)),
            MaxPooling2D((2, 2)),
            Conv2D(64, (3, 3), activation="relu"),
            MaxPooling2D((2, 2)),
            Conv2D(128, (3, 3), activation="relu"),
            MaxPooling2D((2, 2)),
            Conv2D(128, (3, 3), activation="relu"),
            MaxPooling2D((2, 2)),
            Flatten(),
            Dense(512, activation="relu"),
            Dropout(0.5),
            Dense(1, activation="sigmoid")  # ä½¿ç”¨ "softmax" è¿›è¡Œå¤šç±»åˆ†ç±»
        ])

        model.summary()

        # ç¼–è¯‘æ¨¡å‹
        model.compile(
            loss="binary_crossentropy",  # ä½¿ç”¨ "categorical_crossentropy" è¿›è¡Œå¤šç±»åˆ†ç±»
            optimizer=legacy.RMSprop(learning_rate=1e-4),
            metrics=["accuracy"]
        )

        # è®­ç»ƒæ¨¡å‹
        history = model.fit(
            train_generator,
            steps_per_epoch=train_generator.samples // train_generator.batch_size,
            epochs=10,
            validation_data=val_generator,
            validation_steps=val_generator.samples // val_generator.batch_size
        )

        # è¯„ä¼°æ¨¡å‹
        loss, accuracy = model.evaluate(val_generator)
        
        print(f'éªŒè¯æŸå¤±: {loss}')
        print(f'éªŒè¯å‡†ç¡®ç‡: {accuracy}')


if __name__ == "__main__":
    trainer = Trainer(æ–‡ä»¶åç§°="object_model.pb")
    trainer.è¿è¡Œ(æ•°æ®="assets/train", æµ‹è¯•æ•°æ®="assets/test")